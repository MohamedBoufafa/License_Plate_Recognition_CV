{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13119c60",
   "metadata": {},
   "source": [
    "# Recreate `gputest` environment (safe, recommended steps)\n",
    "\n",
    "Run the following commands in a Windows cmd terminal (one block at a time). These commands:\n",
    "- remove the old environment\n",
    "- create a fresh environment\n",
    "- install NumPy with conda (so we avoid DLL/pip conflicts)\n",
    "- install PyTorch wheels built for CUDA 11.3 via PyTorch's index\n",
    "- install the rest of the Python packages via pip (requirements.txt)\n",
    "\n",
    "1) Remove old env and create a new one:\n",
    "\n",
    "```cmd\n",
    "conda deactivate\n",
    "conda env remove -n gputest -y\n",
    "conda create -n gputest python=3.10 -y\n",
    "conda activate gputest\n",
    "```\n",
    "\n",
    "2) Install NumPy (conda):\n",
    "\n",
    "```cmd\n",
    "conda install numpy=1.24.3 -y\n",
    "```\n",
    "\n",
    "3) Install CUDA toolkit (conda) - optional but recommended:\n",
    "\n",
    "```cmd\n",
    "conda install cudatoolkit=11.3 -c conda-forge -y\n",
    "```\n",
    "\n",
    "4) Install PyTorch (pip) with CUDA 11.3 wheels from PyTorch's index:\n",
    "\n",
    "```cmd\n",
    "pip install --extra-index-url https://download.pytorch.org/whl/cu113 \\\n",
    "  torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1\n",
    "```\n",
    "\n",
    "5) Install the rest of the requirements (note: `numpy` is intentionally removed from requirements.txt):\n",
    "\n",
    "```cmd\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "6) Restart the Jupyter kernel and select the `gputest` environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bae178",
   "metadata": {},
   "source": [
    "# Environment Setup with requirements.txt\n",
    "Follow these steps to set up your environment:\n",
    "\n",
    "1. First, remove the existing environment:\n",
    "```bash\n",
    "conda deactivate\n",
    "conda env remove -n gputest -y\n",
    "```\n",
    "\n",
    "2. Create a new environment with Python 3.10:\n",
    "```bash\n",
    "conda create -n gputest python=3.10 -y\n",
    "conda activate gputest\n",
    "```\n",
    "\n",
    "3. Install PyTorch dependencies:\n",
    "```bash\n",
    "conda install pytorch==1.12.1 torchvision==0.13.1 torchaudio==0.12.1 cudatoolkit=11.3 -c pytorch -y\n",
    "```\n",
    "\n",
    "4. Install other requirements:\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "After running these commands, restart the kernel and select the new 'gputest' environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bad107c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01multralytics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m YOLO\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01measyocr\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import easyocr\n",
    "import re\n",
    "from collections import defaultdict, deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99286732",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"license_plate_best.pt\") # finetuned weights and biases .pt means pretrained\n",
    "reader = easyocr.Reader(['en'], gpu=True)\n",
    "\n",
    "# Regex pattern for license plate validation 2 letters, 2 digits, 3 letters (e.g., AB12CDE)\n",
    "plate_pattern = re.compile(r'^[A-Z]{2}[0-9]{2}[A-Z]{3}$')\n",
    "# Regex pattern for Algerian license plates: 5 or 6 digits, followed by 3 digits, followed by 2 digits\n",
    "algerian_pattern = re.compile(r'^[0-9]{5,6}[0-9]{3}[0-9]{2}$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ecbf084c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_plate_format(ocr_text):\n",
    "    mapping_num_to_char = {'0': 'O', '1': 'I', '2': 'Z', '5': 'S', '8': 'B'}\n",
    "    mapping_char_to_num = {v: k for k, v in mapping_num_to_char.items()}\n",
    "    \n",
    "    ocr_text = ocr_text.upper().replace(\" \", \"\")\n",
    "    if len(ocr_text) != 7:\n",
    "        return None # Invalid length for standard plate\n",
    "    corrected = []\n",
    "    for i, char in enumerate(ocr_text):\n",
    "        if i < 2 or i >= 4:  # Letter positions\n",
    "            if char in mapping_num_to_char:\n",
    "                corrected.append(mapping_num_to_char[char])\n",
    "            elif char.isalpha():\n",
    "                corrected.append(char)\n",
    "            else:\n",
    "                return None # Invalid character for letter position\n",
    "        else:  # Digit positions\n",
    "            if char in mapping_char_to_num:\n",
    "                corrected.append(mapping_char_to_num[char])\n",
    "            elif char.isdigit():\n",
    "                corrected.append(char)\n",
    "            else:\n",
    "                return None # Invalid character for digit position\n",
    "    return ''.join(corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5a943e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_plate(plate_crop):\n",
    "    if plate_crop is None or plate_crop.size == 0:\n",
    "        return \"\"\n",
    "    \n",
    "    #preprocess for OCR\n",
    "    gray = cv2.cvtColor(plate_crop, cv2.COLOR_BGR2GRAY)\n",
    "    _, thresh = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    plate_resized = cv2.resize(thresh, None, fx=2, fy=2, interpolation=cv2.INTER_CUBIC)\n",
    "    try:\n",
    "        ocr_result = reader.readtext(plate_resized, detail=0,allowlist='ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789')\n",
    "        if len(ocr_result) > 0:\n",
    "            candidate = correct_plate_format(ocr_result[0])\n",
    "            if candidate and plate_pattern.match(candidate):\n",
    "                return candidate\n",
    "    except:\n",
    "        pass\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51572dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "plate_history = defaultdict(lambda: deque(maxlen=10)) # Store last 10 readings for each plate ID\n",
    "plate_final_counts = {}  # Store final confirmed plate for each detected plate ID\n",
    "def get_box_id(x1,y1,x2,y2):\n",
    "    return f\"{int(x1/10)}_{int(y1/10)}_{int(x2/10)}_{int(y2/10)}\"\n",
    "\n",
    "def get_stable_plate(box_id,new_text):\n",
    "    if new_text:\n",
    "        plate_history[box_id].append(new_text)\n",
    "        most_common = max(set(plate_history[box_id]), key=plate_history[box_id].count)\n",
    "        plate_final_counts[box_id] = most_common\n",
    "    return plate_final_counts.get(box_id, \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12a34718",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_video_path = \"gettyimages-807748644-640_adpp.mp4\"\n",
    "output_video_path = \"output_with_license_plates.mp4\"\n",
    "\n",
    "cap = cv2.VideoCapture(input_video_path) # if input_video_path is 0, it uses webcam\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_video_path, fourcc, cap.get(cv2.CAP_PROP_FPS), (int(cap.get(3)), int(cap.get(4))))\n",
    "\n",
    "CONF_THRESH = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f1107162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy version: 1.24.3\n"
     ]
    }
   ],
   "source": [
    "# Make sure numpy is properly imported and working\n",
    "import numpy as np\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Verify numpy is working\n",
    "print(\"NumPy version:\", np.__version__)\n",
    "\n",
    "# Initialize video capture and output\n",
    "input_video_path = \"gettyimages-807748644-640_adpp.mp4\"\n",
    "output_video_path = \"output_with_license_plates.mp4\"\n",
    "\n",
    "cap = cv2.VideoCapture(input_video_path) # if input_video_path is 0, it uses webcam\n",
    "if not cap.isOpened():\n",
    "    raise ValueError(\"Could not open video file\")\n",
    "\n",
    "# Get video properties\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "# Initialize video writer\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "# Set confidence threshold for detections\n",
    "CONF_THRESH = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "12637c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    results = model(frame,verbose=False)\n",
    "    \n",
    "    for result in results:\n",
    "        boxes = result.boxes\n",
    "        for box in boxes:\n",
    "            conf = float(box.conf.cpu().numpy()[0])\n",
    "            if conf < CONF_THRESH:\n",
    "                continue\n",
    "            \n",
    "            x1, y1, x2, y2 = map(int, box.xyxy.cpu().numpy()[0])\n",
    "            plate_crop = frame[y1:y2, x1:x2]\n",
    "            \n",
    "            text = recognize_plate(plate_crop)\n",
    "            \n",
    "            box_id = get_box_id(x1,y1,x2,y2)\n",
    "            stable_text = get_stable_plate(box_id, text)\n",
    "            \n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 3)\n",
    "            \n",
    "            # Overlay zoomed-in plate above the detected plate\n",
    "            if plate_crop.size > 0:\n",
    "                overlay_h, overlay_w = 150, 400\n",
    "                plate_resized = cv2.resize(plate_crop, (overlay_w, overlay_h))\n",
    "                \n",
    "                oy1 = max(0, y1 - overlay_h - 40)\n",
    "                ox1 = x1\n",
    "                oy2,ox2 = oy1 + overlay_h, ox1 + overlay_w\n",
    "                \n",
    "                if oy2 <= frame.shape[0] and ox2 <= frame.shape[1]:\n",
    "                    frame[oy1:oy2, ox1:ox2] = plate_resized\n",
    "                    \n",
    "                    if stable_text:\n",
    "                        cv2.putText(frame, stable_text, (ox1, oy1 - 20), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 0), 6)\n",
    "                        cv2.putText(frame, stable_text, (ox1, oy1 - 20), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255,255), 3)\n",
    "    out.write(frame)\n",
    "    cv2.imshow(\"License Plate Recognition\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700e5cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"Processing complete. Output saved to\", output_video_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2794266c",
   "metadata": {},
   "source": [
    "# Real-Time License Plate Detection and Recognition\n",
    "This code will:\n",
    "1. Process video input (from file or camera)\n",
    "2. Detect license plates using YOLO model\n",
    "3. Recognize text using EasyOCR\n",
    "4. Display results in real-time\n",
    "5. Save processed video with detections (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27c574e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video(input_path=0, output_path=None, confidence_threshold=0.25):\n",
    "    \"\"\"\n",
    "    Process video for license plate detection and recognition\n",
    "    Args:\n",
    "        input_path: Path to video file or camera index (default 0 for webcam)\n",
    "        output_path: Path to save processed video (optional)\n",
    "        confidence_threshold: Confidence threshold for YOLO detections\n",
    "    \"\"\"\n",
    "    # Initialize video capture\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video source\")\n",
    "        return\n",
    "    \n",
    "    # Get video properties\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    \n",
    "    # Initialize video writer if output path is specified\n",
    "    writer = None\n",
    "    if output_path:\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        writer = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
    "    \n",
    "    # Dictionary to store recognized plates and their counts\n",
    "    plate_counts = defaultdict(int)\n",
    "    # Queue to store recent detections for tracking\n",
    "    recent_plates = deque(maxlen=30)  # Store last 30 frames worth of detections\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            # Run YOLO detection\n",
    "            results = model(frame)[0]\n",
    "            \n",
    "            # Process each detection\n",
    "            for det in results.boxes.data.tolist():\n",
    "                x1, y1, x2, y2, conf, _ = det\n",
    "                \n",
    "                if conf < confidence_threshold:\n",
    "                    continue\n",
    "                \n",
    "                # Extract license plate region\n",
    "                plate_roi = frame[int(y1):int(y2), int(x1):int(x2)]\n",
    "                if plate_roi.size == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Recognize text\n",
    "                ocr_results = reader.readtext(plate_roi)\n",
    "                \n",
    "                for (bbox, text, ocr_conf) in ocr_results:\n",
    "                    # Try to correct and validate the plate format\n",
    "                    corrected_text = correct_plate_format(text)\n",
    "                    if corrected_text:\n",
    "                        plate_counts[corrected_text] += 1\n",
    "                        recent_plates.append(corrected_text)\n",
    "                        \n",
    "                        # Draw bounding box\n",
    "                        cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
    "                        # Add text\n",
    "                        cv2.putText(frame, corrected_text, (int(x1), int(y1)-10),\n",
    "                                  cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "            \n",
    "            # Display most frequent recent detections\n",
    "            y_pos = 30\n",
    "            for plate, count in sorted(plate_counts.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "                text = f\"{plate}: {count}\"\n",
    "                cv2.putText(frame, text, (10, y_pos),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
    "                y_pos += 30\n",
    "            \n",
    "            # Show frame\n",
    "            cv2.imshow('License Plate Detection', frame)\n",
    "            \n",
    "            # Write frame if output path specified\n",
    "            if writer:\n",
    "                writer.write(frame)\n",
    "            \n",
    "            # Break on 'q' press\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "                \n",
    "    finally:\n",
    "        # Clean up\n",
    "        cap.release()\n",
    "        if writer:\n",
    "            writer.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        \n",
    "        # Print final results\n",
    "        print(\"\\nDetected License Plates:\")\n",
    "        for plate, count in sorted(plate_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "            print(f\"{plate}: {count} detections\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c543908f",
   "metadata": {},
   "source": [
    "# Usage Examples\n",
    "Below are examples of how to use the video processing function:\n",
    "1. For webcam input\n",
    "2. For video file input\n",
    "3. For video file input with saving output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d831a04",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.5.4) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:1268: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvDestroyAllWindows'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 38\u001b[0m, in \u001b[0;36mprocess_video\u001b[1;34m(input_path, output_path, confidence_threshold)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Run YOLO detection\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Process each detection\u001b[39;00m\n",
      "File \u001b[1;32me:\\conda\\envs\\gputest\\lib\\site-packages\\ultralytics\\engine\\model.py:101\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calls the 'predict' function with given arguments to perform object detection.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(source, stream, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\conda\\envs\\gputest\\lib\\site-packages\\ultralytics\\engine\\model.py:242\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[1;32m--> 242\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\conda\\envs\\gputest\\lib\\site-packages\\ultralytics\\engine\\predictor.py:196\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[1;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\conda\\envs\\gputest\\lib\\site-packages\\torch\\autograd\\grad_mode.py:43\u001b[0m, in \u001b[0;36m_DecoratorContextManager._wrap_generator.<locals>.generator_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[1;32m---> 43\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[1;32me:\\conda\\envs\\gputest\\lib\\site-packages\\ultralytics\\engine\\predictor.py:255\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[1;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m--> 255\u001b[0m     im \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim0s\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n",
      "File \u001b[1;32me:\\conda\\envs\\gputest\\lib\\site-packages\\ultralytics\\engine\\predictor.py:123\u001b[0m, in \u001b[0;36mBasePredictor.preprocess\u001b[1;34m(self, im)\u001b[0m\n\u001b[0;32m    122\u001b[0m     im \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mascontiguousarray(im)  \u001b[38;5;66;03m# contiguous\u001b[39;00m\n\u001b[1;32m--> 123\u001b[0m     im \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    125\u001b[0m im \u001b[38;5;241m=\u001b[39m im\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Numpy is not available",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Example 1: Use webcam\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# process_video()  # Uncomment to run\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Example 2: Process a video file\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[43mprocess_video\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mZinou\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mOneDrive\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mDesktop\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43m3cs\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mprojects\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mslm2\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43m06_License_Plate_Recognition_CV\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mgettyimages-807748644-640_adpp.mp4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Uncomment and replace with your video path\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Example 3: Process a video file and save the output\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# process_video(\"path_to_your_video.mp4\", \"output_video.mp4\")  # Uncomment and replace with your paths\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[21], line 92\u001b[0m, in \u001b[0;36mprocess_video\u001b[1;34m(input_path, output_path, confidence_threshold)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m writer:\n\u001b[0;32m     91\u001b[0m     writer\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m---> 92\u001b[0m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdestroyAllWindows\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m# Print final results\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mDetected License Plates:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.5.4) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:1268: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvDestroyAllWindows'\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Use webcam\n",
    "# process_video()  # Uncomment to run\n",
    "\n",
    "\n",
    "# Example 2: Process a video file\n",
    "process_video(\"C:\\\\Users\\\\Zinou\\\\OneDrive\\\\Desktop\\\\3cs\\\\projects\\\\slm2\\\\06_License_Plate_Recognition_CV\\\\gettyimages-807748644-640_adpp.mp4\")  # Uncomment and replace with your video path\n",
    "\n",
    "# Example 3: Process a video file and save the output\n",
    "# process_video(\"path_to_your_video.mp4\", \"output_video.mp4\")  # Uncomment and replace with your paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785329d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gputest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
